{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Parallel Processing in Python\n",
      "\n",
      "An obvious way to improve the performance of Python code is to make it run in parallel. Any relatively new computer will have multiple cores, which means that several processors can operate on the same data stored on memory. However, most of the code we have written in the course so far does not take advantage of more than one of them. In addition there is now widespread availability of computing clusters, such as [those offered for use by Amazon](http://aws.amazon.com/ec2/) (Vanderbilt also has [its own cluster](http://www.accre.vanderbilt.edu)). Clusters allow several computers to work together by exchanging data over a network.\n",
      "\n",
      "Parallel computing involves breaking a task into several independent sub-tasks, distributing these sub-tasks to available processors or computers, then coordinating the execution of these tasks and combining their outputs in an appropriate way.\n",
      "\n",
      "There are several different models for parallel processing, including:\n",
      "\n",
      "* **Message passing**: processes or other program components running in parallel communicate by sending and receiving messages, which allows for easy synchronization. \n",
      "* **Multi-threading**: within a single process, some architectures allow for the existence of several threads, which execute independently, though they share the same resources from the process in which they reside.\n",
      "* **Task farming**: a master process delegates independent calculations to available processors (task farm), and collects their outputs when complete.\n",
      "* **Single program, multiple data (SPMD)** Probably the most common type of parallel processing, in which tasks are split up and run simultaneously on multiple processors with different input in order to obtain results faster. All tasks execute their copy of the same program simultaneously.\n",
      "* **Multiple program, multiple data (MPMD)** Like SPMD, except each task may be executing a different program."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## multiprocessing\n",
      "\n",
      "The simplest way (though probably not the best) for performing parallel computing in Python is via the built-in process-based library for concurrent computing, called `multiprocessing`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import multiprocessing\n",
      "import os\n",
      "import numpy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def task(args):\n",
      "    print \"PID =\", os.getpid(), \", args =\", args\n",
      "    \n",
      "    return os.getpid(), args"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "task(\"test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PID = 49964 , args = test\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "(49964, 'test')"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pool = multiprocessing.Pool(processes=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = pool.map(task, [1,2,3,4,5,6,7,8])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PID = 49978 , args = 1\n",
        "PID = 49979 , args = 2\n",
        "PID = 49980 , args = 3\n",
        "PID = 49981 , args = 4\n",
        "PID = 49978 , args = 5\n",
        "PID = 49981 , args = 8\n",
        "PID = 49980 , args = 7\n",
        "PID = 49978 , args = 6\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(49978, 1),\n",
        " (49979, 2),\n",
        " (49980, 3),\n",
        " (49981, 4),\n",
        " (49978, 5),\n",
        " (49978, 6),\n",
        " (49980, 7),\n",
        " (49981, 8)]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The multiprocessing package is very useful for highly parallel tasks that do not need to communicate with each other, other than when sending the initial data to the pool of processes and when and collecting the results. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## IPython parallel\n",
      "\n",
      "The IPython architecture consists of four components, which reside in the `IPython.parallel` package:\n",
      "\n",
      "1. **Engine** The IPython engine is a Python instance that accepts Python commands over a network connection.  When multiple engines are started, parallel and distributed computing becomes possible. An important property of an IPython engine is that it blocks while user code is being executed. \n",
      "\n",
      "2. **Hub** The hub keeps track of engine connections, schedulers, clients, as well as persist all task requests and results in a database for later use.\n",
      "\n",
      "3. **Schedulers** All actions that can be performed on the engine go through a Scheduler. While the engines themselves block when user code is run, the schedulers hide that from the user to provide a fully asynchronous interface to a set of engines.\n",
      "\n",
      "4. **Client** The primary object for connecting to a cluster.\n",
      "\n",
      "![IPython architecture](images/ipython_architecture.png)\n",
      "(courtesy Min Ragan-Kelley)\n",
      "\n",
      "IPython includes a very interesting and versatile parallel computing environment, which is very easy to use. It builds on the concept of ipython engines and controllers, that one can connect to and submit tasks to. To get started using this framework for parallel computing, one first have to start up an IPython cluster of engines. The easiest way to do this is to use the `ipcluster` command,\n",
      "\n",
      "    $ ipcluster start -n 4\n",
      "\n",
      "Or, alternatively, from the \"Clusters\" tab on the IPython notebook dashboard page. This will start 4 IPython engines on the current host, which is useful for multicore systems. It is also possible to setup IPython clusters that spans over many nodes in a computing cluster. For more information about possible use cases, see the official documentation [Using IPython for parallel computing](http://ipython.org/ipython-doc/dev/parallel/).\n",
      "\n",
      "To use the IPython cluster in our Python programs or notebooks, we start by creating an instance of `IPython.parallel.Client`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cli = Client()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the 'ids' attribute we can retreive a list of ids for the IPython engines in the cluster:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cli.ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[0, 1, 2, 3]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each of these engines are ready to execute tasks. We can selectively run code on individual engines:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getpid():\n",
      "    \"\"\" return the unique ID of the current process \"\"\"\n",
      "    import os\n",
      "    return os.getpid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first try it on the notebook process\n",
      "getpid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "49964"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run it on one of the engines\n",
      "cli[0].apply_sync(getpid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "49983"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run it on ALL of the engines at the same time\n",
      "cli[:].apply_sync(getpid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[49983, 49984, 49990, 49985]"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use this cluster of IPython engines to execute tasks in parallel. The easiest way to dispatch a function to different engines is to define the function with the decorator:\n",
      "\n",
      "    @view.parallel(block=True)\n",
      "\n",
      "Here, `view` is supposed to be the engine pool which we want to dispatch the function (task). Once our function is defined this way we can dispatch it to the engine using the `map` method in the resulting class (in Python, a decorator is a language construct which automatically wraps the function into another function or a class).\n",
      "\n",
      "To see how all this works, lets look at an example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview = cli[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@dview.parallel(block=True)\n",
      "def dummy_task(delay):\n",
      "    \"\"\" a dummy task that takes 'delay' seconds to finish \"\"\"\n",
      "    import os, time\n",
      "\n",
      "    t0 = time.time()\n",
      "    pid = os.getpid()\n",
      "    time.sleep(delay)\n",
      "    t1 = time.time()\n",
      "    \n",
      "    return [pid, t0, t1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate random delay times for dummy tasks\n",
      "delay_times = numpy.random.rand(4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, to map the function `dummy_task` to the random delay time data, we use the `map` method in `dummy_task`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dummy_task.map(delay_times)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "in sync results <function __call__ at 0x10fe415f0>\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "[[49983, 1376072756.274707, 1376072756.651694],\n",
        " [49984, 1376072756.276985, 1376072756.330785],\n",
        " [49990, 1376072756.279287, 1376072756.34464],\n",
        " [49985, 1376072756.280688, 1376072756.698575]]"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's do the same thing again with many more tasks and visualize how these tasks are executed on different IPython engines:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def visualize_tasks(results):\n",
      "    res = numpy.array(results)\n",
      "    fig, ax = subplots(figsize=(10, res.shape[1]))\n",
      "    \n",
      "    yticks = []\n",
      "    yticklabels = []\n",
      "    tmin = min(res[:,1])\n",
      "    for n, pid in enumerate(numpy.unique(res[:,0])):\n",
      "        yticks.append(n)\n",
      "        yticklabels.append(\"%d\" % pid)\n",
      "        for m in numpy.where(res[:,0] == pid)[0]:\n",
      "            ax.add_patch(Rectangle((res[m,1] - tmin, n-0.25),\n",
      "                         res[m,2] - res[m,1], 0.5, color=\"green\", alpha=0.5))\n",
      "        \n",
      "    ax.set_ylim(-.5, n+.5)\n",
      "    ax.set_xlim(0, max(res[:,2]) - tmin + 0.)\n",
      "    ax.set_yticks(yticks)\n",
      "    ax.set_yticklabels(yticklabels)\n",
      "    ax.set_ylabel(\"PID\")\n",
      "    ax.set_xlabel(\"seconds\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "delay_times = numpy.random.rand(64)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = dummy_task.map(delay_times)\n",
      "visualize_tasks(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-32-002199129e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvisualize_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/ipython-1.0.0_dev-py2.7.egg/IPython/parallel/client/remotefunction.pyc\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, *sequences)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/ipython-1.0.0_dev-py2.7.egg/IPython/parallel/client/remotefunction.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *sequences)\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/ipython-1.0.0_dev-py2.7.egg/IPython/parallel/client/remotefunction.pyc\u001b[0m in \u001b[0;36msync_view_results\u001b[0;34m(f, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_sync_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_sync_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/ipython-1.0.0_dev-py2.7.egg/IPython/parallel/client/remotefunction.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *sequences)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnparts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "in sync results <function __call__ at 0x10fe415f0>\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's a nice and easy parallelization! We can see that we utilize all four engines quite well.\n",
      "\n",
      "But one short coming so far is that the tasks are not load balanced, so one engine might be idle while others still have more tasks to work on.\n",
      "\n",
      "However, the IPython parallel environment provides a number of alternative \"views\" of the engine cluster, and there is a view that provides load balancing as well (above we have used the \"direct view\", which is why we called it \"dview\").\n",
      "\n",
      "To obtain a load balanced view we simply use the `load_balanced_view` method in the engine cluster client instance `cli`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lbview = cli.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@lbview.parallel(block=True)\n",
      "def dummy_task_load_balanced(delay):\n",
      "    \"\"\" a dummy task that takes 'delay' seconds to finish \"\"\"\n",
      "    import os, time\n",
      "\n",
      "    t0 = time.time()\n",
      "    pid = os.getpid()\n",
      "    time.sleep(delay)\n",
      "    t1 = time.time()\n",
      "    \n",
      "    return [pid, t0, t1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = dummy_task_load_balanced.map(delay_times)\n",
      "visualize_tasks(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "in sync results <function __call__ at 0x10fe415f0>\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAADXCAYAAABxotffAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYP8fx/ekoFQBv3UAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x110b52950>"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the example above we can see that the engine cluster is a bit more efficiently used, and the time to completion is shorter than in the previous example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MPI \n",
      "\n",
      "When more communication between processes is required, sophisticated solutions such as MPI and OpenMP are often needed. MPI is process based parallel processing library/protocol, and can be used in Python programs through the `mpi4py` package:\n",
      "\n",
      "http://mpi4py.scipy.org/\n",
      "\n",
      "To use the `mpi4py` package we include `MPI` from `mpi4py`:\n",
      "\n",
      "    from mpi4py import MPI\n",
      "\n",
      "A MPI python program must be started using the `mpirun -n N` command, where `N` is the number of processes that should be included in the process group.\n",
      "\n",
      "Note that the IPython parallel enviroment also has support for MPI, but to begin with we will use `mpi4py` and the `mpirun` in the follow examples."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpitest.py\n",
      "\n",
      "from mpi4py import MPI\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "if rank == 0:\n",
      "   data = [1.0, 2.0, 3.0, 4.0]\n",
      "   comm.send(data, dest=1, tag=11)\n",
      "elif rank == 1:\n",
      "   data = comm.recv(source=0, tag=11)\n",
      "    \n",
      "print \"rank =\", rank, \", data =\", data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing mpitest.py\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpirun -n 2 python mpitest.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "rank = 0 , data = [1.0, 2.0, 3.0, 4.0]\r\n",
        "rank = 1 , data = [1.0, 2.0, 3.0, 4.0]\r\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 2\n",
      "\n",
      "Send a numpy array from one process to another:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi-numpy-array.py\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "if rank == 0:\n",
      "   data = numpy.random.rand(10)\n",
      "   comm.Send(data, dest=1, tag=13)\n",
      "elif rank == 1:\n",
      "   data = numpy.empty(10, dtype=numpy.float64)\n",
      "   comm.Recv(data, source=0, tag=13)\n",
      "    \n",
      "print \"rank =\", rank, \", data =\", data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing mpi-numpy-array.py\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpirun -n 2 python mpi-numpy-array.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "rank = 0 , data = [ 0.1390156   0.69297545  0.6407395   0.4230003   0.50780772  0.01070704\r\n",
        "  0.64526132  0.28495787  0.16191392  0.41756738]\r\n",
        "rank = 1 , data = [ 0.1390156   0.69297545  0.6407395   0.4230003   0.50780772  0.01070704\r\n",
        "  0.64526132  0.28495787  0.16191392  0.41756738]\r\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi-matrix-vector.py\n",
      "# example from: http://mpi4py.scipy.org/docs/usrman/tutorial.html\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy\n",
      "\n",
      "def matvec(comm, A, x):\n",
      "    m = A.shape[0] # local rows\n",
      "    p = comm.Get_size()\n",
      "    xg = numpy.zeros(m*p, dtype='d')\n",
      "    comm.Allgather([x,  MPI.DOUBLE], [xg, MPI.DOUBLE])\n",
      "    y = numpy.dot(A, xg)\n",
      "    return y\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "A = numpy.random.rand(5,5)\n",
      "x = numpy.random.rand(5)\n",
      "y_mpi = matvec(comm, A, x)\n",
      "    \n",
      "if rank == 0:\n",
      "\n",
      "    y = numpy.dot(A, x)\n",
      "\n",
      "    print \"y =\", y\n",
      "\n",
      "    print \"y_mpi =\", y_mpi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing mpi-matrix-vector.py\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpirun -n 1 python mpi-matrix-vector.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "y = [ 0.75548869  1.18886841  1.00569608  1.21543953  1.19217254]\r\n",
        "y_mpi = [ 0.75548869  1.18886841  1.00569608  1.21543953  1.19217254]\r\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare some random data\n",
      "a = numpy.random.rand(100)\n",
      "numpy.save(\"random-vector.npy\", a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi-psum.py\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy as np\n",
      "\n",
      "def psum(a):\n",
      "    r = MPI.COMM_WORLD.Get_rank()\n",
      "    size = MPI.COMM_WORLD.Get_size()\n",
      "    m = len(a) / size\n",
      "    locsum = np.sum(a[r*m:(r+1)*m])\n",
      "    rcvBuf = np.array(0.0, 'd')\n",
      "    MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE], [rcvBuf, MPI.DOUBLE], op=MPI.SUM)\n",
      "    return rcvBuf\n",
      "\n",
      "a = np.load(\"random-vector.npy\")\n",
      "s = psum(a)\n",
      "\n",
      "if MPI.COMM_WORLD.Get_rank() == 0:\n",
      "    print \"sum =\", s, \", numpy sum =\", a.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing mpi-psum.py\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpirun -n 4 python mpi-psum.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sum = 46.5692124205 , numpy sum = 46.5692124205\r\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## OpenMP\n",
      "\n",
      "What about OpenMP? OpenMP is a standard and widely used thread-based parallel API that unfortunaltely is **not** useful in Python. The reason is that the CPython implementation use a global interpreter lock, making it impossible to simultaneously run several Python threads. Threads are therefore not useful for parallel computing in Python, unless it is only used to wrap compiled code that do the OpenMP parallelization (Numpy does something like that). \n",
      "\n",
      "This is clearly a limitation in the Python interpreter, and as a consequence all parallelization in Python must use processes (not threads)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## OpenCL\n",
      "\n",
      "OpenCL is an API for heterogenous computing, for example using GPUs for numerical computations. There is a python package called `pyopencl` that allows OpenCL code to be compiled, loaded and executed on the compute units completely from within Python. This is a nice way to work with OpenCL, because the time-consuming computations should be done on the compute units in compiled code, and in this Python only server as a control language. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file opencl-dense-mv.py\n",
      "\n",
      "import pyopencl as cl\n",
      "import numpy\n",
      "import time\n",
      "\n",
      "# problem size\n",
      "n = 10000\n",
      "\n",
      "# platform\n",
      "platform_list = cl.get_platforms()\n",
      "platform = platform_list[0]\n",
      "\n",
      "# device\n",
      "device_list = platform.get_devices()\n",
      "device = device_list[0]\n",
      "\n",
      "if False:\n",
      "    print(\"Platform name:\" + platform.name)\n",
      "    print(\"Platform version:\" + platform.version)\n",
      "    print(\"Device name:\" + device.name)\n",
      "    print(\"Device type:\" + cl.device_type.to_string(device.type))\n",
      "    print(\"Device memory: \" + str(device.global_mem_size//1024//1024) + ' MB')\n",
      "    print(\"Device max clock speed:\" + str(device.max_clock_frequency) + ' MHz')\n",
      "    print(\"Device compute units:\" + str(device.max_compute_units))\n",
      "\n",
      "# context\n",
      "ctx = cl.Context([device]) # or we can use cl.create_some_context()\n",
      "\n",
      "# command queue\n",
      "queue = cl.CommandQueue(ctx)\n",
      "\n",
      "# kernel\n",
      "KERNEL_CODE = \"\"\"\n",
      "//\n",
      "// Matrix-vector multiplication: r = m * v\n",
      "//\n",
      "#define N %(mat_size)d\n",
      "__kernel\n",
      "void dmv_cl(__global float *m, __global float *v, __global float *r)\n",
      "{\n",
      "    int i, gid = get_global_id(0);\n",
      "    \n",
      "    r[gid] = 0;\n",
      "    for (i = 0; i < N; i++)\n",
      "    {\n",
      "        r[gid] += m[gid * N + i] * v[i];\n",
      "    }\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "kernel_params = {\"mat_size\": n}\n",
      "program = cl.Program(ctx, KERNEL_CODE % kernel_params).build()\n",
      "\n",
      "# data\n",
      "A = numpy.random.rand(n, n)\n",
      "x = numpy.random.rand(n, 1)\n",
      "\n",
      "# host buffers\n",
      "h_y = numpy.empty(numpy.shape(x)).astype(numpy.float32)\n",
      "h_A = numpy.real(A).astype(numpy.float32)\n",
      "h_x = numpy.real(x).astype(numpy.float32)\n",
      "\n",
      "# device buffers\n",
      "mf = cl.mem_flags\n",
      "d_A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_A)\n",
      "d_x_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_x)\n",
      "d_y_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=h_y.nbytes)\n",
      "\n",
      "# execute OpenCL code\n",
      "t0 = time.time()\n",
      "event = program.dmv_cl(queue, h_y.shape, None, d_A_buf, d_x_buf, d_y_buf)\n",
      "event.wait()\n",
      "cl.enqueue_copy(queue, h_y, d_y_buf)\n",
      "t1 = time.time()\n",
      "\n",
      "print \"opencl elapsed time =\", (t1-t0)\n",
      "\n",
      "# Same calculation with numpy\n",
      "t0 = time.time()\n",
      "y = numpy.dot(h_A, h_x)\n",
      "t1 = time.time()\n",
      "\n",
      "print \"numpy elapsed time =\", (t1-t0)\n",
      "\n",
      "# see if the results are the same\n",
      "print \"max deviation =\", numpy.abs(y-h_y).max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing opencl-dense-mv.py\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python opencl-dense-mv.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "opencl elapsed time = 0.273945808411\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "numpy elapsed time = 0.0477759838104\r\n",
        "max deviation = 0.0168457\r\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cython Parallel\n",
      "\n",
      "Before running the next cell, make sure you have first started your cluster, you can use the [clusters tab in the dashboard](/#tab2) to do so.  Because this example transfers lots of large arrays, we recommend that you first configure your cluster to use the 'NoDB' hub messaging support, which removes a few features but has the lowest memory footprint.  You can do so by putting in your IPython profile directory a file called `ipcontroller_config.py` that contains simply:\n",
      "\n",
      "    # Configuration file for ipcontroller.\n",
      "    c = get_config()\n",
      "    # The class to use for the DB backend\n",
      "    c.HubFactory.db_class = 'IPython.parallel.controller.dictdb.NoDB'\n",
      "\n",
      "See [the IPython docs](http://ipython.org/ipython-doc/dev/parallel/parallel_db.html?#cost) for further details."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "rc = Client()\n",
      "dv = rc[:]\n",
      "dv.block = True\n",
      "dv.activate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we load the cython magic on all engines and execute the cython magic as well on all engines:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px %load_ext cythonmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "%%cython -lm -lgsl -lgslcblas\n",
      "\n",
      "cimport cython\n",
      "import numpy as np\n",
      "from numpy cimport *\n",
      "\n",
      "cdef extern from \"math.h\":\n",
      "    double sqrt(double) \n",
      "  \n",
      "cdef extern from \"gsl/gsl_rng.h\":\n",
      "    ctypedef struct gsl_rng_type\n",
      "    ctypedef struct gsl_rng\n",
      "\n",
      "    gsl_rng_type *gsl_rng_mt19937\n",
      "    gsl_rng *gsl_rng_alloc(gsl_rng_type * T) nogil\n",
      "  \n",
      "cdef extern from \"gsl/gsl_randist.h\":\n",
      "    double gamma \"gsl_ran_gamma\"(gsl_rng * r,double,double)\n",
      "    double gaussian \"gsl_ran_gaussian\"(gsl_rng * r,double)\n",
      "  \n",
      "cdef gsl_rng *r = gsl_rng_alloc(gsl_rng_mt19937)\n",
      "\n",
      "@cython.wraparound(False)\n",
      "@cython.boundscheck(False)\n",
      "def gibbs(int N=20000,int thin=500):\n",
      "    cdef: \n",
      "        double x=0\n",
      "        double y=0\n",
      "        int i, j\n",
      "        ndarray[float64_t, ndim=2] samples\n",
      "\n",
      "    samples = np.empty((N,thin))\n",
      "    for i from 0 <= i < N:\n",
      "        for j from 0 <= j < thin:\n",
      "            x = gamma(r,3,1.0/(y*y+4))\n",
      "            y = gaussian(r,1.0/sqrt(x+1))\n",
      "        samples[i,0] = x\n",
      "        samples[i,1] = y\n",
      "    return samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Divide the array by the number of nodes.  In this case they divide evenly, a more general partitioning of sizes is easy to do as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 1000\n",
      "thin = 10\n",
      "n = N/len(rc.ids)\n",
      "dv.push(dict(n=n, thin=thin))\n",
      "# Let's just confirm visually we got what we expect\n",
      "dv['n']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can time purely the execution of the gibbs sampler on the remote nodes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit\n",
      "dv.execute('gibbs(n, thin)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But a more realistic (and costly) benchmark must also include the cost of bringing the results back from the cluster engines to our local namespace.  For that, we assign the call to the variable `a` on each node and then use the view's `gather` method to pull them back in:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit\n",
      "dv.execute('a = gibbs(n, thin)')\n",
      "a = dv.gather('a')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## References\n",
      "\n",
      "[Scientific Python Lectures](http://github.com/jrjohansson/scientific-python-lectures) by Robert Johansson\n",
      "\n",
      "[Using IPython for Parallel Computing](http://ipython.org/ipython-doc/dev/parallel/)\n",
      "\n",
      "[MPI for Python tutorial](http://mpi4py.scipy.org/docs/usrman/tutorial.html) at SciPy\n",
      "\n",
      "[PyOpenCL](http://mathema.tician.de/software/pyopencl)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width:800px;\n",
        "/*        margin-left:16% !important;*/\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: Helvetica, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 130%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "/*    .prompt{\n",
        "        display: None;\n",
        "    }*/\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 16pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "\n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }\n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x10f9b0dd0>"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}