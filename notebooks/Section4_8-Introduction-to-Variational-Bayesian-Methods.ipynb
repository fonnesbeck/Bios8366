{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Variational Bayesian Methods\n",
    "\n",
    "#### This lecture borrows quite heavily from the wonderful exposition from Matthew B at Civis Analytics [here](https://www.civisanalytics.com/blog/variational-inference-ground/). Many thanks to him. \n",
    "\n",
    "The Bayesian Neural Network example (written by Thomas Wiecki) at the end of the lecture and can also be found in the pymc3 docs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In Bayesian analysis, the most common strategy for computing posterior quantities is through Markov Chain Monte Carlo (MCMC). Despite recent advances in efficient sampling, MCMC methods still remain computationally intensive for more than a few thousand observations. A more scalable alternative to sampling is Variational Inference (VI), which re-frames the problem of computing the posterior distribution as a minimization of the Kullback-Leibler divergence between the true posterior and a member of some approximating family. \n",
    "\n",
    "In this lecture, we provide a basic overview of the VI framework as well as practical examples of its implementation using the Automatic Differentiation Variational Inference (ADVI) engine in PyMC3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Variational Bayesian Methods \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, what's the bottleneck for Bayesian methods?\n",
    "\n",
    "Suppose we have data $\\boldsymbol{x}$ that is dependent on some latent variables $\\theta$. The inferential problem is to compute the conditional density of the latent variables given the \n",
    "observations, $p(\\theta | x)$. From this probability we can compute all sorts of useful quantities. Conditional probability gives us \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\theta|x) = \\frac{p(\\theta,x)}{p(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Where \n",
    "\\begin{equation}\n",
    "p(x) = \\int p(\\theta,x) d\\theta,\n",
    "\\end{equation}\n",
    "\n",
    "Which is the log density of the data, often called the \"model evidence.\"\n",
    "- This quantity is often unavailable in closed form and is the main source of computational headache, hearbreak, and frustration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what can we do instead??\n",
    "\n",
    "\n",
    "1. Divide and Conquer approaches: See [this paper](https://arxiv.org/abs/1311.4780) or [this paper](https://arxiv.org/abs/1508.05880) for more details\n",
    "\n",
    "2. One alternative approach is to build an approximation to the posterior $p(\\theta|X)$ using some other distribution $q(\\theta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we go about approximating a distribution with another?\n",
    "\n",
    "### Example: Approximating a Student's t with a gaussian \n",
    "Let's approximate a Students-t distribution with $\\nu =3$  with a Gaussian distribution of some mean and variance.\n",
    "\n",
    "One naive approach would be to build a set of test points and minimize the MSE between the $\\log p(z)$ and $\\log q(z)$. \n",
    "\n",
    "NOTE THIS IS NOT A GOOD WAY TO DO THIS. \n",
    "\n",
    "$$\n",
    "{\\hat \\phi} = \\underset{\\phi}{{\\rm arg\\,min}} \\frac{\\sum_{i} q(\\theta_{i};\\phi)\\left[\\log q(\\theta_{i};\\phi) - \\log p(\\theta_{i})\\right]^{2}}{\\sum_{i} q(\\theta_{i};\\phi)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our packages:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import scipy.special\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "%matplotlib inline \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Okay let's define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ν = 3\n",
    "ν_NORM = scipy.special.gammaln((ν + 1)/2) - scipy.special.gammaln(ν/2) - 0.5 * np.log(ν*np.pi)\n",
    "def logp(z):\n",
    "    \"\"\"log of students-t dist.\"\"\"\n",
    "    return -0.5 * (ν+1) * np.log(1 + z*z/ν) + ν_NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5.0, 5.0, 1000)\n",
    "pz = np.exp(logp(z))\n",
    "plt.figure()\n",
    "plt.plot(z, pz, label='p(z)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define some functions to implement our routine for the approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's weight points that have more probability in the approximation $q(z)$, the idea being that points near the bulk of  $q(\\theta)$ are more important to get right. OK. How do we pick $\\theta_i$ ? Well, let's use the known distribution of  $q(\\theta;\\phi)$  at each step of the optimization to select a grid of points which sample the regions of highest probability density. Then we will optimize the objective function that we defined in terms of MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Approximating Distro\n",
    "def logq(θ, mu, lnsigma):\n",
    "    \"\"\"log of Gaussian parameterized by mean and log(sigma)\"\"\"\n",
    "    sigma = np.exp(lnsigma)\n",
    "    return -0.5 * ((θ - mu) / sigma) ** 2 - lnsigma - 0.5*np.log(2.0 * np.pi)\n",
    "## Objective function and fit using scipy optimizer\n",
    "def regression_vi(logp, n, mu_start, lnsigma_start, atol=1e-6):\n",
    "    \"\"\"use an optimizer for simple 1D VI\"\"\"\n",
    "    phi_start = np.array([mu_start, lnsigma_start])\n",
    "    \n",
    "    # Objective function. Computes sum above on a grid.\n",
    "    def obj(phi):\n",
    "        _sigma = np.exp(phi[1])  # get sigma\n",
    "        \n",
    "        # This is the grid, factor of 10 is a random choice.\n",
    "        z = np.linspace(phi[0] - 10.0*_sigma , phi[0] + 10.0*_sigma, n)\n",
    "\n",
    "        # Build weights and differences.\n",
    "        logqz = logq(z, phi[0], phi[1])\n",
    "        w = np.exp(logqz)\n",
    "        diff = logqz - logp(z)\n",
    "        return np.sum(diff * diff * w) / np.sum(w)\n",
    "\n",
    "    # Run the optimizer.\n",
    "    opts = {'disp': True, 'maxiter': 5000, 'maxfev': 5000,\n",
    "            'fatol': atol, 'xatol': 1e-8}\n",
    "    phi_hat = scipy.optimize.minimize(obj, phi_start,\n",
    "                                      method='Nelder-Mead',\n",
    "                                      options=opts)\n",
    "    print(phi_hat)\n",
    "    return phi_hat['x'], phi_hat\n",
    "\n",
    "phi_hat, res = regression_vi(logp, 100, 100.0, -100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = np.linspace(-5.0, 5.0, 1000)\n",
    "pθ = np.exp(logp(θ))\n",
    "qθ = np.exp(logq(θ, phi_hat[0], phi_hat[1]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(θ, pθ, label='p(θ)')\n",
    "plt.plot(θ, qθ, label='q(θ)')\n",
    "plt.xlabel('θ')\n",
    "plt.ylabel('PDF')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a better solution would be to minimize the KL divergence between the approximating distribution and the truth:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "D_{\\rm KL}\\big(Q||P\\big) &=& \\int q(\\theta) \\log\\frac{q(\\theta)}{p(\\theta)}d\\theta\\\\\n",
    "&=& \\int q(\\theta)\\log q(\\theta)d\\theta - \\int q(\\theta)\\log p(\\theta)d\\theta\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_vi(logp, n, mu_start, lnsigma_start):\n",
    "    \"\"\"vi with KL divergence\"\"\"\n",
    "    phi_start = np.array([mu_start, lnsigma_start])\n",
    "    \n",
    "    # Objective function. Computes the KL div of q and p.\n",
    "    def obj(phi):\n",
    "        # This term is -\\int q*log(q).\n",
    "        # Also known as the differential entropy.\n",
    "        # For a Gaussian, it can be computed exactly. \n",
    "        # See wikipedia or something.\n",
    "        entropy = phi[1] + 0.5*np.log(2.0 * np.pi) + 0.5 ## \n",
    "        \n",
    "        # now we need to evaluate the second integral in the KL divergence, let's numerically approximate it with a sum\n",
    "        # This is the grid, factor of 20 is a random choice.\n",
    "        _sigma = np.exp(phi[1])  # get sigma        \n",
    "        θ = np.linspace(phi[0] - 20.0*_sigma , phi[0] + 20.0*_sigma, n) #number of grid points \n",
    "        dθ = θ[1] - θ[0]  # factor needed for numerical integral\n",
    "        \n",
    "        # This term is \\int q*log(p)\n",
    "        logqθ = logq(θ, phi[0], phi[1]) #just evaluate the variational density at each z on the grid and \n",
    "                                        # at the current value of phi\n",
    "        qθ = np.exp(logqθ) # back transform\n",
    "        return -entropy - np.sum(qθ * logp(θ) * dθ) #KL divergence for this phi\n",
    "    # Pass this objective function to a scipy optimizer\n",
    "    phi_hat = scipy.optimize.minimize(obj, phi_start,\n",
    "                                      method='Nelder-Mead',\n",
    "                                      options={'disp': True})\n",
    "    print(phi_hat)\n",
    "    return phi_hat['x'], phi_hat\n",
    "\n",
    "phi_hat, res = kl_vi(logp, 10000, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = np.linspace(-5.0, 5.0, 1000)\n",
    "pθ = np.exp(logp(θ))\n",
    "qθ = np.exp(logq(θ, phi_hat[0], phi_hat[1]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(θ, pθ, label='p(θ)')\n",
    "plt.plot(θ, qθ, label='q(θ)')\n",
    "plt.xlabel('θ')\n",
    "plt.ylabel('PDF')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at a more difficult pdf to approximate:\n",
    "$$\n",
    " \\log p(\\theta) = 10^{3}\\log \\theta + \\log(1-\\theta) - c\n",
    "$$\n",
    "\n",
    "where the constant $c = Beta(10^3+1, 2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive approach here would be to fit a rescaled Gaussian to match the support of $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logq_unit(θ, mu, lnsigma):\n",
    "    \"\"\"log of Gaussian parameterized by mean and log(sigma)\n",
    "    has unit integral over 0,1 \n",
    "    and value zero outside of 0,1\n",
    "    \"\"\"\n",
    "    val = np.zeros_like(θ)\n",
    "    msk = (θ >= 1.0) | (θ <= 0.0)\n",
    "    val[msk] = -np.inf\n",
    "    if np.any(~msk):\n",
    "        sigma = np.exp(lnsigma)\n",
    "        a, b = (0.0 - mu) / sigma, (1.0 - mu) / sigma\n",
    "        val[~msk] = scipy.stats.truncnorm.logpdf(θ[~msk], a=a, b=b, loc=mu, scale=sigma)\n",
    "    \n",
    "    return val\n",
    "\n",
    "def logp_hard(θ, a=1e3, b=1):\n",
    "    val = np.zeros_like(θ)\n",
    "    msk = (θ >= 1.0) | (θ <= 0.0)\n",
    "    val[msk] = -np.inf\n",
    "    if np.any(~msk):\n",
    "        val[~msk] = a * np.log(θ) + b * np.log(1.0 - θ) - scipy.special.betaln(a + 1.0, b + 1.0)\n",
    "    return val\n",
    "\n",
    "def kl_vi_unit(logp, n, mu_start, lnsigma_start, eps=1e-8):\n",
    "    \"\"\"vi with KL divergence over unit integral\"\"\"\n",
    "    phi_start = np.array([mu_start, lnsigma_start])\n",
    "    \n",
    "    # Objective function. Computes the KL div of q and p.\n",
    "    def obj(phi):\n",
    "        # This term is -\\int q*log(q).\n",
    "        sigma = np.exp(phi[1])\n",
    "        a, b = (0.0 - phi[0]) / sigma, (1.0 - phi[0]) / sigma\n",
    "        entropy = scipy.stats.truncnorm.entropy(a=a, b=b, loc=phi[0], scale=sigma)\n",
    "\n",
    "        # This is the grid, factor of 20 is a random choice.\n",
    "        _sigma = np.exp(phi[1])  # get sigma        \n",
    "        θ = np.linspace(eps, 1.0 - eps, n)\n",
    "        dθ= θ[1] - θ[0]  # factor needed for numerical integral\n",
    "        \n",
    "        # This term is \\int q*log(p)\n",
    "        logqθ = logq_unit(θ, phi[0], phi[1])\n",
    "        qθ = np.exp(logqθ)\n",
    "\n",
    "        return -entropy - np.sum(qθ * logp(θ) * dθ)\n",
    "\n",
    "    # Run the optimizer.\n",
    "    phi_hat = scipy.optimize.minimize(obj, phi_start,\n",
    "                                      method='Nelder-Mead',\n",
    "                                      options={'disp': True, 'maxfev': 10000})\n",
    "    print(phi_hat)\n",
    "    return phi_hat['x'], phi_hat\n",
    "\n",
    "phi_hat, res = kl_vi_unit(logp_hard, 10000, 0.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = np.linspace(0.5, 0.999999, 100000)\n",
    "pθ = np.exp(logp_hard(θ))\n",
    "qθ = np.exp(logq_unit(θ, phi_hat[0], phi_hat[1]))\n",
    "dθ_dlogitθ = θ * (1.0 - θ)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scipy.special.logit(θ), pθ * dθ_dlogitθ, label='p(logit(θ))')\n",
    "plt.plot(scipy.special.logit(θ), qθ * dθ_dlogitθ, label='q(logit(θ))')\n",
    "plt.xlabel('logit(θ)')\n",
    "plt.ylabel('PDF')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a little weird and rescaling a gaussian to squeeze into the support of $\\theta$ is not really a good solution if your parameters all have different supports.\n",
    "\n",
    "Instead, let's do the reverse and transform the parameter space to that of a gaussian.\n",
    "A transformation $T$ that maps $[0,1] \\to R$ is the logit function. \n",
    "$$\n",
    "T(\\theta)=\\zeta = {\\rm logit}(\\theta) = \\log\\left(\\frac{\\theta}{1-\\theta}\\right)\\\\\n",
    "{\\rm logit}^{-1}(\\zeta) = {\\rm sigmoid}(\\zeta) = \\frac{1}{1 + \\exp\\left(-\\zeta\\right)}\\\\\n",
    "\\frac{d{\\rm sigmoid}(\\zeta)}{d\\zeta} = {\\rm sigmoid}(\\zeta) \\left[1 - {\\rm sigmoid}(\\zeta)\\right]\\\\\n",
    "$$\n",
    "and then our pdf in terms of the transformed parameter is given by\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\log p_{\\zeta}(\\zeta) &=& \\log p({\\rm sigmoid}(\\zeta)) + \\log\\left|\\frac{d{\\rm sigmoid}(\\zeta)}{d\\zeta}\\right|\\\\\n",
    "&=& \\log p({\\rm sigmoid}(\\zeta)) + \\log {\\rm sigmoid}(\\zeta) + \\log(1-{\\rm sigmoid}(\\zeta))\\ .\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_easy(logitθ, a=1e3, b=1):\n",
    "    logabsjac = -1.0 * (np.log(1.0 + np.exp(-logitθ)) + np.log(1.0 + np.exp(logitθ)))\n",
    "    return (-a * np.log(1.0 + np.exp(-logitθ)) - b * np.log(1.0 + np.exp(logitθ)) + \n",
    "            logabsjac - \n",
    "            scipy.special.betaln(a + 1.0, b + 1.0))\n",
    "\n",
    "phi_hat, res = kl_vi(logp_easy, 10000, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitθ = np.linspace(0.0, 14.0, 100000)\n",
    "plogitθ = np.exp(logp_easy(logitθ))\n",
    "qlogitθ = np.exp(logq(logitθ, phi_hat[0], phi_hat[1]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(logitθ, plogitθ, label='p(logit(θ))')\n",
    "plt.plot(logitθ, qlogitθ, label='q(logit(θ))')\n",
    "plt.xlabel('logit(z)')\n",
    "plt.ylabel('PDF')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what did we learn here? \n",
    "\n",
    "Re-casting $\\theta$ to have the same support as a Gaussian makes for nice and easy minimization and fits much better than the other way around. \n",
    "\n",
    "# KEEP THIS IN MIND FOR LATER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### This is all fine and dandy, but wait a second .....I thought we didn't know the posterior! How do we optimize the $D_{KL}$ when we don't have the true posterior??\n",
    "\n",
    "Great question, hypothetical student! Let's revisit the KL divergence and see if we can employ some math trickery:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi^*=\\arg\\min_{\\phi\\in\\Phi}D_{\\rm KL}(q(\\theta; \\phi) || p(\\theta|X))\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now, we see that the KL divergence is given by\n",
    "\n",
    "\\begin{eqnarray}\n",
    "D_{\\rm KL}\\big(Q||P\\big)&=& \\int q(\\theta) \\log\\frac{q(\\theta)}{p(\\theta|x)}d\\theta\\\\\n",
    "&=& E_q\\left[\\log\\frac{q(\\theta)}{p(\\theta|x)}\\right]\\\\\n",
    "&=&E_q[\\log q(\\theta)]-E_q[\\log p(\\theta|x)]\\\\\n",
    "&=&-E_q[\\log p(\\theta,x)]+E_q[\\log q(\\theta)]+\\log p(x)\\\\\n",
    "&=& -ELBO +\\log p(x)\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can see that the KL divergence is given by the sum of the ELBO and Model Evidence. To see why, let's revisit the model evidence. \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log p(x)&=&\\log\\int p(x,\\theta)d\\theta\\\\\n",
    "&=&\\log\\int p(x,\\theta)\\frac{q(\\theta)}{q(\\theta)}d\\theta\\\\\n",
    "&=&\\log(E_{q}\\left[\\frac{p(x,\\theta)}{q(\\theta)}\\right])\\\\\n",
    "&\\geq& E_q[\\log p(x,\\theta)]-E_q[\\log q(\\theta)].\n",
    "\\end{eqnarray*}\n",
    "\n",
    "What this implies for a computational solution is that minimizing the KL divergence is accomplished by maximizing the evidence lower bound.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [Automatic Differentiation Variational Inference](https://arxiv.org/abs/1603.00788): A Tale of Two Transformations\n",
    "With ease of computation in mind, Kucukelbir et. al. 2015 developed a way to perform VI automatically. Without going into too much detail (see reference for details), the authors proposed transforming the problem in a series of steps:\n",
    "\n",
    "1. Specify the joint model, $p(x,\\theta)$\n",
    "2. Transform model into surrogate containing unconstrained real-valued latent variables, $\\zeta$. $p(x,\\theta) \\to p(x,\\zeta)$\n",
    "    - Variational inference is then performed on the transformed model. New objective: \n",
    "    \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\phi^*=\\arg\\min_{\\phi\\in\\Phi} KL(q(\\zeta; \\phi)||p(\\zeta|x))\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "all latent variables are defined on the same space. ADVI can now use a single variational family for all models\n",
    " \n",
    "3. ADVI recasts the gradient of the variational objective function as an expectation over $q$. This allows for the use of Monte Carlo integration to perform the optimization\n",
    "4. Next, the framework transforms the problem again and re-casts the gradient in terms of a standard Gaussian distribution. This makes MC integration very efficient since sampling is done from $N(0,1)$\n",
    "5. Compute noisy gradients to optimize the objective. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get a little bit of a feel for how this happens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we're first going to transform all of our parameters to have the same support in $R^k$. We next need to optimize the KL divergence on the transformed space (like in the sigmoid transformation above). \n",
    "\n",
    "To accomplish this, we need to optimize the ELBO for the transformed objective. Our objective function for the transformed variables is now given by the ELBO of the transformation:\n",
    "\n",
    "$$\n",
    "ELBO(\\phi) = E_{q(\\zeta;\\phi)}\\left[\\log p(x,T^{-1}(\\zeta))+\\log|\\det J_{T^{-1}}(\\zeta)|\\right] - E_q[\\log q(\\zeta;\\phi)]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notation is starting to get a litle hairy, so let's return to basics. Remember, the first expectation is just an expectation over a joint likelihood in terms of the approximating variational distribution.  \n",
    "\n",
    "$$\n",
    "\\int q(z)\\log p(x,T^{-1}(\\zeta))|\\det J_{T^{-1}}(\\zeta)| dz \\approx \\frac{1}{n}\\sum_i \\log p(x|\\zeta_{i})p(\\zeta_{i})\n",
    "$$\n",
    "\n",
    "Problem is, we can't differentiate this MC integration with respect to the variational parameters (since they parameterize the distribution from which we draw $\\zeta_i$). So we need to transform one more time so that the expectation is in terms of a standard normal (this is called elliptic standardization), and them perform MC integration with a draw from a standard normal. \n",
    "$$\n",
    "\\int q(\\zeta) \\log p(x|\\zeta)p(\\zeta) d\\zeta = \\int N(\\zeta) \\log p(x|\\zeta\\sigma+\\mu)p(\\zeta\\sigma+\\mu) dz \\approx \\frac{1}{n}\\sum_i \\log p(x|\\zeta_{i}\\sigma+\\mu)p(\\zeta_{i}\\sigma+\\mu)\\ .\n",
    "$$\n",
    "\n",
    "Now, since this integral is explicitly in terms of the variational parameters, we can optimize our objective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above is something we can take derivatives of using a computational backend. We'll use pymc3 in a moment. But let's return to our weird pdf.\n",
    "\n",
    "We can now implement a really crude version of advi on our weird pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlogp_easy_dζ(ζ):\n",
    "    θ = scipy.special.expit(ζ)\n",
    "    return (1e3 + 1.0) * (1.0 - θ) - 2 * θ\n",
    "\n",
    "def kl_vi_with_sgd(dlogp, mu, lnsigma, rng, n_iter=100000, n_draw=1, eta=5e-4, eta_decay=5e-5):\n",
    "    for i in range(n_iter):\n",
    "        # Draw the points and convert back to draws from the variational approximation.\n",
    "        ζ_standard = rng.normal(size=n_draw)\n",
    "        sigma = np.exp(lnsigma)\n",
    "        ζ = ζ_standard*sigma + mu\n",
    "        \n",
    "        # Compute the derivs.\n",
    "        dkl_dmu = -np.mean(dlogp(ζ))\n",
    "        dkl_dlnsigma = -np.mean(dlogp(ζ) * ζ_standard * sigma) - 1\n",
    "        \n",
    "        # Now do SGD with the KL divergence.\n",
    "        lnsigma -= dkl_dlnsigma * eta\n",
    "        mu -= dkl_dmu * eta\n",
    "        \n",
    "        # Decay the learning rate.\n",
    "        eta *= 1.0 - eta_decay\n",
    "        \n",
    "        if i % (n_iter // 25) == 0 or i == n_iter - 1:\n",
    "            print(\"iter, mu, lnsigma: % 7d|% 10.4e|% 10.4e\" % \n",
    "                  (i, mu, lnsigma))\n",
    "        \n",
    "    return np.array([mu, lnsigma])\n",
    "\n",
    "rng = np.random.RandomState(seed=5678)\n",
    "phi_hat = kl_vi_with_sgd(dlogp_easy_dζ, 0.0, 0.0, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitθ = np.linspace(0.0, 14.0, 100000)\n",
    "plogitθ = np.exp(logp_easy(logitθ))\n",
    "qlogitθ = np.exp(logq(logitθ, phi_hat[0], phi_hat[1]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(logitθ, plogitθ, label='p(logit(θ))')\n",
    "plt.plot(logitθ, qlogitθ, label='q(logit(θ))')\n",
    "plt.xlabel('logit(θ)')\n",
    "plt.ylabel('PDF')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we certainly don't want to have to compute derivatives all by hand!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference in PyMC3\n",
    "\n",
    "Theano, PyTorch, and Tensorflow are all computational backends to compute derivatives symbolically so we don't have to hand code anything. PyMC3 uses Theano which is what we will be working in.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create toy data\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "size = 50\n",
    "true_intercept = 1\n",
    "true_slope = 2\n",
    "x = np.linspace(0, 1, size)\n",
    "# y = a + b*x\n",
    "true_regression_line = true_intercept + true_slope * x\n",
    "# add noise\n",
    "y = true_regression_line + np.random.normal(scale=.5, size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\n",
    "#plt.plot(x, y,\"bo\", label='sampled data', alpha =0.01)\n",
    "ax.plot(x, y,\"bo\", label='sampled data',alpha =0.5)\n",
    "ax.plot(x, true_regression_line, label='true regression line', lw=2.,color = \"red\")\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some very basic pymc3 coding:\n",
    "with pm.Model() as model:\n",
    "    # Define priors\n",
    "    sigma = pm.HalfCauchy('sigma', beta=10, testval=1.)\n",
    "    intercept = pm.Normal('Intercept', 0, sd=20)\n",
    "    x_coeff = pm.Normal('x', 0, sd=20)\n",
    "    # Define likelihood\n",
    "    likelihood = pm.Normal('y', mu=intercept + x_coeff * x,\n",
    "                        sd=sigma, observed=y)\n",
    "    trace = pm.sample(3000, njobs=2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "pm.traceplot(trace[100:])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, cool now for the sake of demonstration, let's ramp up the number of observations and see how to perform minibatch ADVI in pymc3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500000\n",
    "true_intercept = 1\n",
    "true_slope = 2\n",
    "x = np.linspace(0, 1, size)\n",
    "# y = a + b*x\n",
    "true_regression_line = true_intercept + true_slope * x\n",
    "# add noise\n",
    "y = true_regression_line + np.random.normal(scale=.5, size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First, we need to set up a framework where we minibatch the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 5000\n",
    "x_minibatch = pm.Minibatch(x, batch_size=batchsize)\n",
    "y_minibatch = pm.Minibatch(y, batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now just need to make a few minor adjustments to our model to accomodate these new tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "with pm.Model() as model_vi: # \n",
    "    # Define priors\n",
    "    sigma = pm.HalfCauchy('sigma', beta=10, testval=1.)\n",
    "    intercept = pm.Normal('Intercept', 0, sd=20)\n",
    "    x_coeff = pm.Normal('x', 0, sd=20)\n",
    "    #define model mean in terms of minibatch tensor\n",
    "    mean = intercept + x_coeff * x_minibatch\n",
    "    # Define likelihood\n",
    "    likelihood = pm.Normal('likelihood', mu = mean, sd = sigma, observed=y_minibatch, total_size = len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_vi:\n",
    "    approx = pm.variational.fit(n=20000, method=\"advi\")#, obj_optimizer =pm.adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(approx.hist[5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_vi = approx.sample(10000) \n",
    "pm.traceplot(trace_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `trace_vi` acts exactly like a trace from mcmc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an in depth introduction to Neural Networks, see Prof. Fonnesbeck's notebook [here](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section6_7-Neural-Networks.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)\n",
    "X = scale(X)\n",
    "X = X.astype(floatX)\n",
    "Y = Y.astype(floatX)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')\n",
    "sns.despine(); ax.legend()\n",
    "ax.set(xlabel='X', ylabel='Y', title='Toy binary classification data set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the model. Since, PyMC3 uses Theano as a back end, functions need to be compiled first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick: Turn inputs and outputs into shared variables.\n",
    "# It's still the same thing, but we can later change the values of the shared variable\n",
    "# (to switch in the test-data later) and pymc3 will just use the new data.\n",
    "# Kind-of like a pointer we can redirect.\n",
    "# For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "ann_input = theano.shared(X_train)\n",
    "ann_output = theano.shared(Y_train)\n",
    "\n",
    "n_hidden = 5\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "init_1 = np.random.randn(X.shape[1], n_hidden).astype(floatX)\n",
    "init_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "init_out = np.random.randn(n_hidden).astype(floatX)\n",
    "\n",
    "with pm.Model() as neural_network:\n",
    "    # Weights from input to hidden layer\n",
    "    weights_in_1 = pm.Normal('w_in_1', 0, sd=1,\n",
    "                             shape=(X.shape[1], n_hidden),\n",
    "                             testval=init_1)\n",
    "\n",
    "    # Weights from 1st to 2nd layer\n",
    "    weights_1_2 = pm.Normal('w_1_2', 0, sd=1,\n",
    "                            shape=(n_hidden, n_hidden),\n",
    "                            testval=init_2)\n",
    "\n",
    "    # Weights from hidden layer to output\n",
    "    weights_2_out = pm.Normal('w_2_out', 0, sd=1,\n",
    "                              shape=(n_hidden,),\n",
    "                              testval=init_out)\n",
    "\n",
    "    # Build neural-network using tanh activation function\n",
    "    act_1 = pm.math.tanh(pm.math.dot(ann_input,\n",
    "                                     weights_in_1))\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1,\n",
    "                                     weights_1_2))\n",
    "    act_out = pm.math.sigmoid(pm.math.dot(act_2,\n",
    "                                          weights_2_out))\n",
    "\n",
    "    # Binary classification -> Bernoulli likelihood\n",
    "    out = pm.Bernoulli('out',\n",
    "                       act_out,\n",
    "                       observed=ann_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with neural_network:\n",
    "    advi_approx = pm.fit(n=10000,method ='advi', obj_optimizer=pm.adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = advi_approx.sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the optimization course by plotting the ELBO values over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(advi_approx.hist)\n",
    "plt.ylabel('-ELBO')\n",
    "plt.xlabel('iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can predict new values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace shared variables with testing set\n",
    "ann_input.set_value(X_test)\n",
    "ann_output.set_value(Y_test)\n",
    "\n",
    "# Creater posterior predictive samples\n",
    "ppc = pm.sample_ppc(trace, model=neural_network, samples=500)\n",
    "\n",
    "# Use probability of > 0.5 to assume prediction of class 1\n",
    "pred = ppc['out'].mean(axis=0) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\n",
    "ax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\n",
    "sns.despine()\n",
    "ax.set(title='Predicted labels in testing set', xlabel='X', ylabel='Y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy = {}%'.format((Y_test == pred).mean() * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.mgrid[-3:3:100j,-3:3:100j].astype(floatX)\n",
    "grid_2d = grid.reshape(2, -1).T\n",
    "dummy_out = np.ones(grid.shape[1], dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_input.set_value(grid_2d)\n",
    "ann_output.set_value(dummy_out)\n",
    "\n",
    "# Creater posterior predictive samples\n",
    "ppc = pm.sample_ppc(trace, model=neural_network, samples=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "contour = ax.contourf(*grid, ppc['out'].mean(axis=0).reshape(100, 100), cmap=cmap)\n",
    "ax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\n",
    "ax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\n",
    "cbar.ax.set_ylabel('Posterior predictive mean probability of class label = 0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "contour = ax.contourf(*grid, ppc['out'].std(axis=0).reshape(100, 100), cmap=cmap)\n",
    "ax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\n",
    "ax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\n",
    "cbar.ax.set_ylabel('Uncertainty (posterior predictive standard deviation)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References: \n",
    "\n",
    "1. Variational Inference from the ground up: https://www.civisanalytics.com/blog/variational-inference-ground/\n",
    "2. [Automatic Differentiation Variational Inference. Kucukelbir, A., Tran D., Ranganath, R., Gelman, A., and Blei, D. M. (2016)](https://arxiv.org/abs/1603.00788)\n",
    "2. [Variational Inference: A Review for Statisticians, David M. Blei, Alp Kucukelbir, Jon D. McAuliffe (2016)] (https://arxiv.org/abs/1601.00670)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
